### Lecture: 5 Generative Adversarial Networks (GANs).
#### Types of Generative Models: 
-  1. Explicit Density (i.e VAE, Autoregressive Neural Networks)
-  2. Implicit Density (i.e GANs)
### GANs parts:
- Generators (take random sample vectir from latent space z and generate image)
- Discriminator( a binary classifier i.e. Real vs fake)
- The Goal of generator is to generate images which are similar to real images and it is not able to be distinguished by Discriminiator.( to fool Discriminator)
- The Goal of Discriminator is to distinguish images generated by Generator(Fake Image) from real images.
- Generator try to generate images which look real.
- latent random varable (Z) --->  Guassian Distribution
- Two loss:
      -  1. Discriminator loss (Binary cross entropy) --J(d)
      -  2. Generator loss-  opposite of D (-J(D))
-Training:
  - Minimax Game:
  - G minimize probability that D is correct
  - Equilibrium is saddle point of discriminator loss
- Good Training: Genertor loss should go down with time and discriminator loss should increase with time
- Curves are constant with time 

- problems with GAN:
   - Global structures (structures which look like real world images but are not)
   - GANs are dificult to evaluate
   - Do they memorize or generalize ?
   ### Inception Score(IS) -measure of GAN(
   - Measure Salienc and diversity:
   - Check how accurate the classfier can recognizethe generated images
   - Saliency: Check whether the generated images can be classified with high confidence.(High score on single class)
   - Diversity: check whether we optain samples form all classes.
  
  #### GAN Training Hacks:
  -  normalize the input between -1 and 1
  -  tanh as last layer of the generator output
  -  No Brainer
  -  sample z from gaussian (shperical z)
  -  Use Batch Norm
  -  avoid sparse gradient
  -  use leaku ReLu
  -  Downsample --> Avg Pooling , conv+stride
  -  Upsample--> deconv+stride, pixelshuffle
  -  Exponential average weights (Problem if noise in D due to SGD)
 ### Common problems with GAN:
 - 1. Vanishing Gradient: if discriminator is too good, then generator training will fail due to vanishing gradient:
       - Remedies: Wasserstein loss, modifiy minimax loss
 - 2. Model Collapse:  owever, if a generator produces an especially plausible output, the generator may learn to produce only that output. In fact, the generator is always trying to find the one output that seems most plausible to the discriminator.
 -   -- Generator outputs only small type of images
 -  3. Failure to converge

### Variations of GANs:
- 1. Progressive GANs: In a progressive GAN, the generator's first layers produce very low resolution images, and subsequent layers add details. 
- 2. Conditional GANs:  Conditional GANs train on a labeled data set and let you specify the label for each generated instance. For example, an unconditional MNIST GAN would produce random digits, while a conditional MNIST GAN would let you specify which digit the GAN should generate. 
      - " add semantic meaning to latent space
- 3. Image-to-Image Translation:- Image-to-Image translation GANs take an image as input and map it to a generated output image with different properties. For example, we can take a mask image with blob of color in the shape of a car, and the GAN can fill in the shape with photorealistic car details

- 4. CycleGAN :- CycleGANs learn to transform images from one set into images that could plausibly belong to another set. For example, a CycleGAN produced the righthand image below when given the lefthand image as input. It took an image of a horse and turned it into an image of a zebra.
- 5. Text-to-image synthesis: Text-to-image GANs take text as input and produce images that are plausible and described by the text. For example, the flower image below was produced by feeding a text description to a GAN.
- 6. Super-Resolution:-
- 7. Face Inpainting:- GANs have been used for the semantic image inpainting task. In the inpainting task, chunks of an image are blacked out, and the system tries to fill in the missing chunks
- 8- Text to speech
- 



 -     
- **New Objective functions (GAN Types)**
- Energy based GAN (EBGAN)
- Least Square GAN (LSGAN)
- Boundary Equilibrium GAN (BEGAN)
-  WGAN

###EBGAN :
- Discriminator is energy based

### DCGAN:
- Deep convolution GAN
- Take random vector z and produces images by upsampling (Deconv)--- similar to decoding for image construction
-------------------------------------------------------------------------------------------------------------------------------------------

## Lecture: 06 Generative Adversarial Networks (GAN - part:02
### GAN Architecture:
- Multi-Scale GAN: Discriminator works at every scale 
- Ex: Progressive GANs: Generator produces small image(low resolution) in begining (4x4) then grow in each iteration 8x8 --> 16x16 --->..........1024x1024
- GAN- Manifold (a+b-c)  sungrasses man + female - man = sunglasses female
- cGAN (sketch to image)
- iGAN (interactive GANs): input image(i.e shoes) (Project on manifold)--> change/manipulate the images (size) ---> reproject to original (reconstruct)
     - projecting image to manifold
-

- 
- 
- 


-  








